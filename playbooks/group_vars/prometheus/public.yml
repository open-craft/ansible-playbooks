---

# GENERAL #####################################################################

COMMON_SERVER_NO_BACKUPS: true

SANITY_CHECK_LIVE_PORTS:
  - host: localhost
    port: 9090
    message: "Cannot connect to Prometheus web port."

# PROMETHEUS ##################################################################

prometheus_web__listen_address: 127.0.0.1:9090

prometheus_config_global_scrape_interval: 30s
prometheus_config_global_evaluation_interval: 30s
prometheus_config_global_scrape_timeout: 10s

prometheus_storage__tsdb__retention: 30d

# The defaults weren't working, though they actually should.  We are not using the Prometheus
# consoles for now, but leaving these settings here ensures that we don't have to debug the
# problem again once we start using them.
prometheus_web__console__templates: "{{ prometheus_install_dir }}/prometheus-{{ prometheus_version }}.{{ prometheus_platform_architecture }}/consoles"
prometheus_web__console__libraries: "{{ prometheus_install_dir }}/prometheus-{{ prometheus_version }}.{{ prometheus_platform_architecture }}/console_libraries"

PROMETHEUS_ALERT_CPU_THRESHHOLD: 50
PROMETHEUS_ALERT_MEMORY_THRESHOLD: 90
PROMETHEUS_ALERT_DISK_SPACE_THRESHOLD: 80

PROMETHEUS_ALERT_HTTP_RESPONSE_TIME_THRESHOLD: 1
PROMETHEUS_ALERT_HTTP_RESPONSE_FRONTEND_ERROR_THRESHOLD: 70

PROMETHEUS_ALERT_HAPROXY_NODES_REGEX: "~\".+\""

prometheus_rules:
  - name: NodeRules
    rules:
    - alert: "instance_down"
      # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0/
      expr: "avg_over_time(up[5m]) < 0.91"
      for: "0s"
      labels:
        severity: "critical"
        environment: "[[ $labels.environment ]]"
      annotations:
        summary: "Instance [[ $labels.node ]] ([[ $labels.instance ]]) down."
        description: "[[ $labels.node ]] ([[ $labels.instance ]]) of job [[ $labels.job ]] is down."
    - alert: "cpu_usage_high"
      expr: "(100 * (1 - (rate(node_cpu{mode='idle'}[5m])))) > {{ PROMETHEUS_ALERT_CPU_THRESHHOLD }}"
      for: "5m"
      labels:
        severity: "warning"
        environment: "[[ $labels.environment ]]"
      annotations:
        summary: "Instance [[ $labels.node ]] ([[ $labels.instance ]]) CPU usage high."
        description: "CPU usage of [[ $labels.node ]] ([[ $labels.instance ]]) of job [[ $labels.job ]] has been above {{ PROMETHEUS_ALERT_CPU_THRESHHOLD }} for 5 minutes."
    - alert: "low_memory"
      expr: "(100 * (1 - (node_memory_MemAvailable / node_memory_MemTotal))) > {{ PROMETHEUS_ALERT_MEMORY_THRESHOLD }}"
      for: "5m"
      labels:
        severity: "warning"
        environment: "[[ $labels.environment ]]"
      annotations:
        summary: "Instance [[ $labels.node ]] ([[ $labels.instance ]]) memory usage high."
        description: "[[ $labels.node ]] ([[ $labels.instance ]]) of job [[ $labels.job ]] memory usage has been above {{ PROMETHEUS_ALERT_MEMORY_THRESHOLD }}% for 5 minutes."
    - alert: "low_disk_space"
      expr: "(100 * (1 - (node_filesystem_free / node_filesystem_size))) > {{ PROMETHEUS_ALERT_DISK_SPACE_THRESHOLD }}"
      for: "5m"
      labels:
        severity: "warning"
        environment: "[[ $labels.environment ]]"
      annotations:
        summary: "Instance [[ $labels.node ]] ([[ $labels.instance ]]) disk usage high."
        description: "Instance disk is more than {{ PROMETHEUS_ALERT_DISK_SPACE_THRESHOLD }}% full."
    - alert: "disk_will_fill_in_4_hours"
      # https://www.robustperception.io/reduce-noise-from-disk-space-alerts/
      expr: "(100 * (1 - (predict_linear(node_filesystem_free{fstype='ext4'}[1h], 4 * 3600)/node_filesystem_size{fstype='ext4'}))) > {{ PROMETHEUS_ALERT_DISK_SPACE_THRESHOLD }}"
      for: "5m"
      labels:
        severity: "warning"
        environment: "[[ $labels.environment ]]"
      annotations:
        summary: "Instance [[ $labels.node ]] ([[ $labels.instance ]]) disk is filling up."
        description: "Instance disk may fill up {{ PROMETHEUS_ALERT_DISK_SPACE_THRESHOLD }}% in 4 hours."
  - name: "HAProxyRules"
    rules:
    - alert: "haproxy_down"
      expr: "avg_over_time(haproxy_up [5m]) < 0.91"
      for: "0s"
      labels:
        severity: "critical"
        environment: "[[ $labels.environment ]]"
      annotations:
        summary: "HAProxy [[ $labels.node ]] ([[ $labels.instance ]]) down."
        description: "[[ $labels.node ]] ([[ $labels.instance ]]) of job [[ $labels.job ]] is down."
    - alert: "haproxy_backend_response_time_high"
      expr: "haproxy_backend_http_response_time_average_seconds{node='{{ PROMETHEUS_ALERT_HAPROXY_NODES_REGEX }}'} > {{ PROMETHEUS_ALERT_HTTP_RESPONSE_TIME_THRESHOLD }}"
      for: "5m"
      labels:
        severity: "warning"
        environment: "[[ $labels.environment ]]"
      annotations:
        summary: "Instance [[ $labels.node ]] ([[ $labels.instance ]]) http response time high."
        description: "HTTP response time of [[ $labels.node ]] ([[ $labels.instance ]]) of job [[ $labels.job ]] has been above {{ PROMETHEUS_ALERT_HTTP_RESPONSE_TIME_THRESHOLD }} for 5 minutes."
    - alert: "haproxy_frontend_request_errors_high"
      expr: "(100 * (haproxy_frontend_request_errors_total{node='{{ PROMETHEUS_ALERT_HAPROXY_NODES_REGEX }}'}/haproxy_frontend_http_requests_total)) > {{ PROMETHEUS_ALERT_HTTP_RESPONSE_FRONTEND_ERROR_THRESHOLD }}"
      for: "5m"
      labels:
        severity: "warning"
        environment: "[[ $labels.environment ]]"
      annotations:
        summary: "Instance [[ $labels.node ]] ([[ $labels.instance ]]) http response time high."
        description: "HTTP response time of [[ $labels.node ]] ([[ $labels.instance ]]) of job [[ $labels.job ]] has been above {{ PROMETHEUS_ALERT_HTTP_RESPONSE_TIME_THRESHOLD }} for 5 minutes."
  - name: "MySQLRules"
    rules:
    - alert: "mysql_down"
      expr: "avg_over_time(mysql_up[5m]) < 0.91"
      for: "0s"
      labels:
        severity: "critical"
        environment: "[[ $labels.environment ]]"
      annotations:
        summary: "MySQL instance [[ $labels.node ]] ([[ $labels.instance ]]) down."
        description: "[[ $labels.node ]] ([[ $labels.instance ]]) of job [[ $labels.job ]] is down."

# ALERT MANAGER ################################################################

prometheus_alert_manager_web__listen_address: "127.0.0.1:9093"

prometheus_alert_manager_data__retention: "300h0m0s"

prometheus_alert_manager_config_global:
  smtp_smarthost: "{{ PROMETHEUS_ALERT_MANAGER_SMTP_SMARTHOST }}"
  smtp_from: "{{ PROMETHEUS_ALERT_MANAGER_SMTP_FROM }}"
  smtp_require_tls: false

prometheus_alert_manager_config_receivers:
  - name: "default"
    email_configs:
    - send_resolved: true
      to: "{{ PROMETHEUS_ALERT_MANAGER_SMTP_TO }}"
  - name: "pager"
    pagerduty_configs:
    - send_resolved: true
      service_key: "{{ PROMETHEUS_ALERT_MANAGER_PAGERDUTY_SERVICE_KEY }}"

prometheus_alert_manager_config_route:
  receiver: "default"
  group_wait: "30s"
  group_interval: "5m"
  repeat_interval: "4h"
  routes:
    - receiver: "pager"
      group_wait: "0s"
      group_interval: "1m"
      repeat_interval: "5m"
      match:
        environment: "prod"
        severity: "critical"

# CONSUL ######################################################################

consul_service_config:
  service:
    name: prometheus
    tags: ["prometheus"]
    port: 9090
    enable_tag_override: true

# FILEBEAT ####################################################################

filebeat_prospectors:
  - fields:
      type: prometheus
    paths:
      - /var/log/prometheus.log*
